{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CONFIG] Instalações e pacotes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema (os, makedirs, glob, listdir, load_dotenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "import os\n",
    "import glob\n",
    "from os import listdir\n",
    "\n",
    "### Para pegar os arquivos .env\n",
    "# from pathlib import Path  # Python 3.6+ only\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv_path='../.env') # Pega da pasta acima dessa\n",
    "def load_dotenv():\n",
    "    \"\"\"Load the .env file normally and also from the current directory.\"\"\"\n",
    "    dotenv.load_dotenv()\n",
    "    dotenv.load_dotenv(dotenv.find_dotenv(usecwd=True))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "## Pega o var_caminho da variável de ambiente\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, os.environ.get('var_caminho_fonte'))\n",
    "\n",
    "var_caminho = os.environ.get('var_caminho_fonte')\n",
    "# var_caminho\n",
    "\n",
    "\n",
    "## Pega bibliotecas de caminhos diversos\n",
    "# import importlib\n",
    "# try:\n",
    "    # importlib.reload(sys.modules['modulo'])\n",
    "# except:\n",
    "    # pass\n",
    "# from modulo import funcao"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Básicos (numpy, math, display, locale, time, random, re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install jupyter\n",
    "\n",
    "\n",
    "# !python -m pip install IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import math\n",
    "\n",
    "# !python -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "import locale\n",
    "# locale.setlocale(locale.LC_ALL, \"pt_BR.UTF-8\")  # Use \"\" for auto, or force e.g. to \"en_US.UTF-8\"\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "from pandas.tseries.offsets import BDay # para os dias úteis\n",
    "# today = datetime.datetime.today()\n",
    "# print(today - BDay(4)) # 4 dias úteis atrás\n",
    "\n",
    "# # !python -m pip install random\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# !python -m pip install regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e análise de dados (Excel, Pandas, Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install findspark\n",
    "\n",
    "# !python -m pip install openpyxl\n",
    "# import openpyxl\n",
    "\n",
    "# !python -m pip install xlsxwriter\n",
    "# import xlsxwriter\n",
    "\n",
    "# !python -m pip install xlrd\n",
    "# import xlrd\n",
    "\n",
    "# !python -m pip install python-calamine\n",
    "# import python_calamine\n",
    "\n",
    "\n",
    "# !python -m pip install pandas\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "# pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive (para ler e escrever arquivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PARA A LEITURA DOS DADOS EM PLANILHA DE SHEETS\n",
    "# # https://stackoverflow.com/questions/71686960/typeerror-credentials-need-to-be-from-either-oauth2client-or-from-google-auth\n",
    "# from google.colab import drive\n",
    "# from google.colab import auth\n",
    "# # !pip install gspread\n",
    "# import gspread\n",
    "# from google.auth import default\n",
    "# creds, _ = default()\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# gc = gspread.authorize(creds)\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização (matplotlib, seaborn, plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n",
    "\n",
    "# !python -m pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# !python -m pip install plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# !python -m pip install graphviz\n",
    "import graphviz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automação (selenium, schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install selenium\n",
    "# !python -m pip install schedule\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import schedule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install scikit-learn\n",
    "!python -m pip install -U imbalanced-learn\n",
    "!python -m pip install kneed\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterização (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install scikit-learn\n",
    "!python -m pip install -U imbalanced-learn\n",
    "!python -m pip install kneed\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, silhouette_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visão computacional (OpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install opencv-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séries temporais (statsmodels, Prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "!python -m pip install prophet\n",
    "from prophet import Prophet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neurais (tensorflow, keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finanças (yfinance, mplfinance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/yfinance/\n",
    "# https://github.com/ranaroussi/yfinance/wiki/Ticker\n",
    "\n",
    "!python -m pip install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "!python -m pip install mplfinance\n",
    "import mplfinance as mpf\n",
    "\n",
    "# # Em R\n",
    "# # https://cran.r-project.org/web/packages/BatchGetSymbols/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de dados em sites e APIs (Beautiful Soup e requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# !python -m pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresão (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de arquivos com ID no Drive (_le_arquivos_drive_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def le_arquivos_drive(id_arquivo):\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    !python -m pip install PyDrive# &> /dev/null\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "\n",
    "    # id_arquivo = \"1sgkZW8mQCGdG_4DicIXFTCn3d-RBoRIk\" # Altere o token dos dados aqui\n",
    "\n",
    "    download = drive.CreateFile({'id': id_arquivo})\n",
    "\n",
    "    download.GetContentFile('file.csv')\n",
    "    dados  = pd.read_csv(\"file.csv\")\n",
    "\n",
    "    return dados\n",
    "    # dados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de arquivos em uma pasta local (_empilha_arquivos_pasta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empilha_arquivos_pasta(\n",
    "    caminho_pasta,\n",
    "    formato, \n",
    "    sep = \",\",\n",
    "    imprimir_nomes_arquivos = True, \n",
    "    adicionar_arquivo_base = True,\n",
    "    imprimir_check = True,\n",
    "    remover_duplicatas = False,\n",
    "    nome_aba = None,\n",
    "    palavra_chave_arquivo = None,\n",
    "    ):\n",
    "    \n",
    "    bd = pd.DataFrame()\n",
    "    check = 0\n",
    "    arquivos_nao_lidos = []\n",
    "\n",
    "    for filename in os.listdir(caminho_pasta):\n",
    "        if ( (filename.endswith(\".\" + formato.lower())) | (formato == \"\") ) * (palavra_chave_arquivo.lower() in filename.lower()):\n",
    "\n",
    "            if imprimir_nomes_arquivos == True:\n",
    "                print(\"Lendo arquivo \" + filename)\n",
    "\n",
    "            try:\n",
    "                if formato.lower() == \"csv\":\n",
    "                    try:\n",
    "                        try:\n",
    "                            novoArquivo = pd.read_csv(caminho_pasta + \"\\\\\" + filename, sep = sep, encoding = \"utf-8\", engine = \"python\")\n",
    "                        except:\n",
    "                            novoArquivo = pd.read_csv(caminho_pasta + \"\\\\\" + filename, encoding = \"utf-8\", engine = \"python\")\n",
    "                    except:\n",
    "                        try:\n",
    "                            novoArquivo = pd.read_csv(caminho_pasta + \"\\\\\" + filename, sep = sep, encoding='iso-8859-1', engine = \"python\")\n",
    "                        except:\n",
    "                            novoArquivo = pd.read_csv(caminho_pasta + \"\\\\\" + filename, encoding='iso-8859-1', engine = \"python\")\n",
    "                            \n",
    "                elif formato.lower() == \"xlsx\":\n",
    "                    if nome_aba is None:\n",
    "                        novoArquivo = pd.read_excel(caminho_pasta + \"\\\\\" + filename)\n",
    "                    else:\n",
    "                        novoArquivo = pd.read_excel(caminho_pasta + \"\\\\\" + filename, sheet_name = nome_aba)\n",
    "\n",
    "\n",
    "                if adicionar_arquivo_base == True:\n",
    "                    novoArquivo[\"Arquivo\"] = filename\n",
    "\n",
    "                print(\"Leitura do arquivo \" + filename + \" concluída (\" + str(len(novoArquivo)) + \" linhas)\")\n",
    "                check = check + len(novoArquivo)\n",
    "                bd = pd.concat([bd, novoArquivo])\n",
    "                # print(\"Leitura do arquivo \" + filename + \" concluída (\" + str(len(novoArquivo)) + \" linhas)\")\n",
    "            \n",
    "            except:\n",
    "                print(\"Erro no arquivo \" + filename)\n",
    "                arquivos_nao_lidos.append(filename)\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if imprimir_check == True:\n",
    "        if (len(bd) == check):\n",
    "            check = \"ok\"\n",
    "        else:\n",
    "            check = \"nok\"\n",
    "        print(\"Check: \" + check + \", \" + str(len(bd))) \n",
    "\n",
    "    if remover_duplicatas == True:\n",
    "        bd = bd.drop_duplicates()\n",
    "\n",
    "    return [bd, arquivos_nao_lidos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [bd_empilhada, arquivos_nao_lidos] = empilha_arquivos_pasta(\n",
    "#     caminho_pasta = r'C:\\Users\\ricardopeloi\\pasta',\n",
    "#     formato = 'xlsx', \n",
    "#     # sep = \",\",\n",
    "#     imprimir_nomes_arquivos = True, \n",
    "#     adicionar_arquivo_base = True,\n",
    "#     imprimir_check = True,\n",
    "#     remover_duplicatas = False,\n",
    "#     nome_aba = None,\n",
    "#     )\n",
    "\n",
    "# bd_empilhada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lê arquivo do Google Sheets em CSV (_convert_google_sheet_url_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_google_sheet_url(url):\n",
    "    # Regular expression to match and capture the necessary part of the URL\n",
    "    pattern = r'https://docs\\.google\\.com/spreadsheets/d/([a-zA-Z0-9-_]+)(/edit#gid=(\\d+)|/edit.*)?'\n",
    "\n",
    "    # Replace function to construct the new URL for CSV export\n",
    "    # If gid is present in the URL, it includes it in the export URL, otherwise, it's omitted\n",
    "    replacement = lambda m: f'https://docs.google.com/spreadsheets/d/{m.group(1)}/export?' + (f'gid={m.group(3)}&' if m.group(3) else '') + 'format=csv'\n",
    "\n",
    "    # Replace using regex\n",
    "    new_url = re.sub(pattern, replacement, url)\n",
    "\n",
    "    return new_url\n",
    "\n",
    "# caminho = \"https://docs.google.com/spreadsheets/d/1FO7R8HkhqqHfVgup5-52DTDKT_roiVpBARNTlgxchCo/edit#gid\"\n",
    "# base = pd.read_csv(convert_google_sheet_url(caminho))\n",
    "# base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostra da base (_amostra_da_base_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amostra_da_base(bd, qtd_por_grupo, taxa_de_conversao, random_state = 1):\n",
    "    quantidade = min(int(round(qtd_por_grupo/taxa_de_conversao, 0)), len(bd))\n",
    "    \n",
    "    if random_state == False:\n",
    "        return bd.sample(n = quantidade)\n",
    "    else:\n",
    "        return bd.sample(n = quantidade, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamentos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimina colunas NA (_elimina_colunas_NA_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_colunas_NA(\n",
    "    base,\n",
    "    imprime_colunas_vazias = False,\n",
    "    imprime_colunas_completas = False,\n",
    "    imprime_colunas_parciais = False,\n",
    "    remove_da_base = True,\n",
    "    retorna_parciais = False\n",
    "    ):\n",
    "\n",
    "    tamanho_da_base = len(base)\n",
    "\n",
    "    colunas_vazias = []\n",
    "    colunas_completas = []\n",
    "    colunas_parciais = []\n",
    "\n",
    "    for coluna in base.columns:\n",
    "        tamanho_da_coluna = len(base[base[coluna].isna()])\n",
    "        if tamanho_da_coluna == tamanho_da_base:\n",
    "            # print(coluna)\n",
    "            colunas_vazias.append(coluna)\n",
    "        elif tamanho_da_coluna == 0:\n",
    "            colunas_completas.append(coluna)\n",
    "        else:\n",
    "            colunas_parciais.append(coluna)\n",
    "\n",
    "    if imprime_colunas_vazias == True:\n",
    "        if len(colunas_vazias) == 0:\n",
    "            print(\"Não há colunas NA\")\n",
    "        else:\n",
    "            print(\"Colunas vazias: \" + str(colunas_vazias))\n",
    "            \n",
    "    if imprime_colunas_completas == True:\n",
    "        if len(colunas_completas) == 0:\n",
    "            print(\"Não há colunas completas\")\n",
    "        else:\n",
    "            print(\"Colunas completas: \" + str(colunas_completas))\n",
    "    \n",
    "    if imprime_colunas_parciais == True:\n",
    "        if len(colunas_parciais) == 0:\n",
    "            print(\"Não há colunas parciais\")\n",
    "        else:\n",
    "            print(\"Colunas parciais: \" + str(colunas_parciais))\n",
    "\n",
    "    if remove_da_base == True:\n",
    "        base = base.drop(colunas_vazias, axis = 1)\n",
    "    \n",
    "    if retorna_parciais == True:\n",
    "        return [base, colunas_parciais]\n",
    "    else:\n",
    "        return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona campo agrupado (_adiciona_campo_agrupado_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adiciona_campo_agrupado(\n",
    "    bd_limpa, bd_unificada, \n",
    "    campos_identificadores, \n",
    "    novo_campo, \n",
    "    imprime_campos = True,\n",
    "    imprime_checks = True\n",
    "    ):\n",
    "    \n",
    "    novo_campos_identificadores = campos_identificadores.copy()\n",
    "    novo_campos_identificadores.append(novo_campo)\n",
    "    if imprime_campos == True:\n",
    "        print(str(novo_campos_identificadores))\n",
    "\n",
    "    bd_limpa_com_campos = bd_limpa[novo_campos_identificadores] \\\n",
    "        .groupby(novo_campos_identificadores).first().reset_index()\n",
    "\n",
    "    bd_limpa_com_campos = bd_limpa_com_campos.groupby(campos_identificadores).last()\n",
    "\n",
    "    if novo_campo in bd_unificada.columns:\n",
    "        bd_unificada_novo = bd_unificada.drop(novo_campo, axis = 1).join(bd_limpa_com_campos, how = \"outer\")\n",
    "    else:\n",
    "        bd_unificada_novo = bd_unificada.join(bd_limpa_com_campos, how = \"outer\")\n",
    "\n",
    "    if imprime_checks == True:\n",
    "        print(novo_campo + \" vazios: \" + str(len(bd_unificada_novo[bd_unificada_novo[novo_campo].isna()])))\n",
    "        print(novo_campo + \" não vazios: \" + str(len(bd_unificada_novo[~bd_unificada_novo[novo_campo].isna()])))\n",
    "\n",
    "    return bd_unificada_novo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completa dígitos, por exemplo padronizar chaves, CPF, etc (_completa_digitos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completa_digitos(texto, qtd):\n",
    "    quantidade_zeros = qtd - len(texto)\n",
    "\n",
    "    contador = quantidade_zeros\n",
    "    while contador > 0:\n",
    "        texto = \"0\" + texto\n",
    "        contador = contador - 1\n",
    "\n",
    "    # print(len(texto))\n",
    "\n",
    "    return texto\n",
    "\n",
    "# bd[\"Ano-Mês\"] = bd[\"Ano\"].astype(str) + \"-\" + bd[\"Mês\"].astype(str).apply(lambda x: completa_digitos(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolida primeiro campo não NA entre duas opções (_consolida_primeiro_campo_existente_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolida_primeiro_campo_existente(campoA, campoB):\n",
    "    try:\n",
    "        import numpy as np\n",
    "\n",
    "        if np.isnan(campoA):\n",
    "            return campoB\n",
    "        else:\n",
    "            return campoA\n",
    "    except:\n",
    "        if pd.isnull(campoA):\n",
    "            return campoB\n",
    "        else:\n",
    "            return campoA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupar cada coluna (_agrupamento_cada_coluna_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupamento_cada_coluna(\n",
    "    bd, \n",
    "    coluna_completa, \n",
    "    colunas_ignoradas = [], \n",
    "    ascending = False,\n",
    "    imprime_tabelas = True,\n",
    "    ):\n",
    "    \n",
    "    from IPython.display import display\n",
    "\n",
    "    campos_com_erro = []\n",
    "    dict_campos = {}\n",
    "\n",
    "    colunas = bd.columns.drop(coluna_completa)\n",
    "\n",
    "    if len(colunas_ignoradas) > 0:\n",
    "        colunas = colunas.drop(colunas_ignoradas)\n",
    "    \n",
    "    for coluna in colunas:\n",
    "        try: \n",
    "            temp_coluna = bd.fillna(\"(vazio)\").groupby(coluna).count()[[coluna_completa]].rename(columns = {coluna_completa: \"Quantidade\"}).sort_values(\"Quantidade\", ascending = ascending)\n",
    "\n",
    "            if ascending == False:\n",
    "                temp_coluna[\"%\"] = temp_coluna[\"Quantidade\"]/temp_coluna[\"Quantidade\"].sum()\n",
    "                temp_coluna[\"% acumulado\"] = temp_coluna[\"%\"].cumsum()\n",
    "            \n",
    "            if imprime_tabelas == True:\n",
    "                display(temp_coluna)\n",
    "                \n",
    "            dict_campos[coluna] = temp_coluna\n",
    "            # limpa(temp_coluna)\n",
    "        \n",
    "        except:\n",
    "            campos_com_erro.append(coluna)\n",
    "            #print(\"Campo \" + coluna + \" deu erro =/\")\n",
    "    \n",
    "    return [campos_com_erro, dict_campos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos de repetições na BD (_exemplos_repeticoes_na_bd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplos_repeticoes_na_bd(\n",
    "    bd_filtro_linhas, \n",
    "    campos_filtro_colunas, \n",
    "    coluna_completa, \n",
    "    ascending = False\n",
    "    ):\n",
    "    \n",
    "    print(\"A tabela possui \" + str(len(bd_filtro_linhas[campos_filtro_colunas])) + \" linhas na base vezes na base\")\n",
    "    \n",
    "    bd_agrupada = bd_filtro_linhas[campos_filtro_colunas+[coluna_completa]] \\\n",
    "        .groupby(campos_filtro_colunas).count() \\\n",
    "        .rename(columns = {coluna_completa: \"Quantidade\"}) \\\n",
    "        .sort_values(\"Quantidade\", ascending = ascending)\n",
    "\n",
    "    return bd_agrupada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De Para com map (_de_para_map_), exemplo com meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mes = {\n",
    "    \"Janeiro\": 1,\n",
    "    \"Fevereiro\": 2,\n",
    "    \"Março\": 3,\n",
    "    \"Abril\": 4,\n",
    "    \"Maio\": 5,\n",
    "    \"Junho\": 6,\n",
    "    \"Julho\": 7,\n",
    "    \"Agosto\": 8,\n",
    "    \"Setembro\": 9,\n",
    "    \"Outubro\": 10,\n",
    "    \"Novembro\": 11,\n",
    "    \"Dezembro\": 12,\n",
    "}\n",
    "# bd[\"Mês\"] = bd[\"Mês\"].map(dict_mes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De Para basicão com loc (_de_para_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_para(tabela_de_para, coluna_de):\n",
    "    try:\n",
    "        return tabela_de_para.loc[coluna_de][0]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenação de colunas (parâmetros), usando uma lista como referência (_concat_parametros_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_parametros(\n",
    "  lista_parametros,\n",
    "  tabela,\n",
    "):\n",
    "\n",
    "  str_parametros = \"\"\n",
    "  for parametro in lista_parametros:\n",
    "    if parametro == lista_parametros[-1]:\n",
    "      str_parametros = str_parametros + str(parametro)\n",
    "\n",
    "    else:\n",
    "      str_parametros = str(parametro) + \" - \" + str_parametros\n",
    "\n",
    "  tabela[str_parametros] = \"\"\n",
    "  for parametro in lista_parametros:\n",
    "    if parametro == lista_parametros[-1]:\n",
    "      tabela[str_parametros] = tabela[str_parametros] + tabela[parametro].astype(str)\n",
    "\n",
    "    else:\n",
    "      tabela[str_parametros] = tabela[parametro].astype(str) + \" - \" + tabela[str_parametros]\n",
    "      # str_parametros = str_parametros + parametro\n",
    "    \n",
    "\n",
    "  # display(tabela)\n",
    "\n",
    "  return (str_parametros, tabela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encaixa um determinado valor entre dois valores que são representados por listas de faixas mínimas e máximas (_encaixa_na_faixa_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encaixa_na_faixa(faixa_minima, faixa_maxima, valor, tipo = \"less-inclusive\"):\n",
    "    faixa_minima.sort()\n",
    "    faixa_maxima.sort()\n",
    "\n",
    "    tabela_personalizada = pd.DataFrame(index = faixa_minima, data = faixa_maxima) \\\n",
    "                                        .reset_index() \\\n",
    "                                        .rename(columns = {\"index\": \"Faixa mínima\", 0: \"Faixa máxima\"}).astype(float)\n",
    "\n",
    "    tabela_personalizada[\"Faixa\"] = tabela_personalizada[\"Faixa mínima\"].astype(str).str.split(\".\").str[0] + \\\n",
    "                                     \" a \" + \\\n",
    "                                     tabela_personalizada[\"Faixa máxima\"].astype(str).str.split(\".\").str[0]\n",
    "    \n",
    "    if tipo == \"more-inclusive\":\n",
    "        \n",
    "        for contador in range(len(tabela_personalizada)):\n",
    "            if (valor > tabela_personalizada.iloc[contador][\"Faixa mínima\"]) & (valor <= tabela_personalizada.iloc[contador][\"Faixa máxima\"]):\n",
    "                # display(tabela_personalizada.iloc[contador])\n",
    "                return tabela_personalizada.iloc[contador][\"Faixa\"]\n",
    "        \n",
    "        return \"Fora das faixas\"\n",
    "\n",
    "    if tipo == \"inclusive\":\n",
    "        \n",
    "        for contador in range(len(tabela_personalizada)):\n",
    "            if (valor >= tabela_personalizada.iloc[contador][\"Faixa mínima\"]) & (valor <= tabela_personalizada.iloc[contador][\"Faixa máxima\"]):\n",
    "                # display(tabela_personalizada.iloc[contador])\n",
    "                return tabela_personalizada.iloc[contador][\"Faixa\"]\n",
    "        \n",
    "        return \"Fora das faixas\"\n",
    "\n",
    "    # if tipo == \"exclusive\": \\\\ mutuamente exclusivo (não tem igual)\n",
    "\n",
    "    else:\n",
    "    # if tipo == \"less-inclusive\":\n",
    "\n",
    "        for contador in range(len(tabela_personalizada)):\n",
    "            if (valor >= tabela_personalizada.iloc[contador][\"Faixa mínima\"]) & (valor < tabela_personalizada.iloc[contador][\"Faixa máxima\"]):\n",
    "                # display(tabela_personalizada.iloc[contador])\n",
    "                return tabela_personalizada.iloc[contador][\"Faixa\"]\n",
    "        \n",
    "        return \"Fora das faixas\"\n",
    "\n",
    "\n",
    "# lista_idade_minima = [float(\"-inf\"), 17, 25, 31, 41, 51]\n",
    "# lista_idade_maxima = [16, 24, 30, 40, 50, float(\"inf\")]\n",
    "\n",
    "# encaixa_na_faixa(lista_idade_minima, lista_idade_maxima, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleciona últimos meses em uma base que tenha coluna de \"Ano-Mês\" (_selecionar_ultimos_meses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecionar_ultimos_meses(bd, coluna_ano_mes, qtd_ultimos_meses = 12):\n",
    "    lista_ano_mes_ordenada = bd[coluna_ano_mes].unique()\n",
    "    lista_ano_mes_ordenada.sort()\n",
    "    # print(lista_ano_mes_ordenada[-qtd_ultimos_meses:])\n",
    "\n",
    "    return bd[bd[coluna_ano_mes].isin(lista_ano_mes_ordenada[-qtd_ultimos_meses:])]\n",
    "\n",
    "# base_DRE_consolidado_12_meses = selecionar_ultimos_meses(base_DRE_consolidado, \"Ano-Mês\", qtd_ultimos_meses = 12)\n",
    "# base_DRE_consolidado_12_meses[[\"Ano-Mês\", \"Faturamento Bruto (R$)\"]].groupby(\"Ano-Mês\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimir o cabeçalho e alguns linhas de bases que tenham muitas colunas (_imprimir_head_todas_colunas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_head_todas_colunas(\n",
    "        bd,\n",
    "        qtd_linhas = 2,\n",
    "        qtd_colunas_display = 10,\n",
    "):\n",
    "    qtd_colunas = len(bd.columns)\n",
    "    for n in range(int(qtd_colunas/qtd_colunas_display)):\n",
    "        display(bd.iloc[:qtd_linhas, (n*qtd_colunas_display):((n+1)*qtd_colunas_display)])\n",
    "\n",
    "    display(bd.iloc[:qtd_linhas, ((n+1)*qtd_colunas_display):])\n",
    "\n",
    "# imprimir_head_todas_colunas(\n",
    "#         bd,\n",
    "#         # qtd_linhas = 2,\n",
    "#         qtd_colunas_display = 12,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise exploratória básica de todos os campos da base - MUITO ÚTIL (_analise_exploratoria_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analise_exploratoria(\n",
    "    bd,\n",
    "    imprime_todas_colunas = False,\n",
    "    imprime_info_colunas = True,\n",
    "    imprime_colunas_vazias = False,\n",
    "    imprime_colunas_completas = False,\n",
    "    imprime_colunas_parciais = False,\n",
    "    remove_da_base = True,\n",
    "    retorna_parciais = False,\n",
    "    # detalhar_colunas_parciais = True,\n",
    "    colunas_ignoradas = []\n",
    "    ):\n",
    "\n",
    "    # COMEÇANDO PELAS COLUNAS DISPONÍVEIS E INFO\n",
    "    if imprime_todas_colunas == True:\n",
    "        display(bd.columns)\n",
    "    \n",
    "    if imprime_info_colunas == True:\n",
    "        for i in range(int(np.ceil(len(bd.columns)/20))):\n",
    "            display(bd.iloc[:, (i*20):min((i+1)*20, len(bd.columns))].info())\n",
    "\n",
    "    # DETALHAMENTO DE QUAIS COLUNAS SÃO NA OU PARCIAIS\n",
    "    if retorna_parciais == True:\n",
    "        [bd_semNA, colunas_parciais] = elimina_colunas_NA(\n",
    "            bd,\n",
    "            imprime_colunas_vazias = imprime_colunas_vazias,\n",
    "            imprime_colunas_completas = imprime_colunas_completas,\n",
    "            imprime_colunas_parciais = imprime_colunas_parciais,\n",
    "            remove_da_base = remove_da_base,\n",
    "            retorna_parciais = True\n",
    "        )\n",
    "\n",
    "        print(colunas_parciais)\n",
    "\n",
    "        # if detalhar_colunas_parciais == True:\n",
    "            # for coluna in colunas_parciais:\n",
    "                # print(coluna)\n",
    "                # print(\"# \" + coluna + \": \" + str(len(colunas_parciais[colunas_parciais[coluna].isna()])))\n",
    "    else:\n",
    "        colunas_parciais = []\n",
    "\n",
    "        bd_semNA = elimina_colunas_NA(\n",
    "            bd,\n",
    "            imprime_colunas_vazias = imprime_colunas_vazias,\n",
    "            imprime_colunas_completas = imprime_colunas_completas,\n",
    "            imprime_colunas_parciais = imprime_colunas_parciais,\n",
    "            remove_da_base = remove_da_base,\n",
    "            retorna_parciais = retorna_parciais\n",
    "        )\n",
    "\n",
    "    [campos_com_erro, colunas_agrupadas] = agrupamento_cada_coluna(\n",
    "        bd.reset_index(), \n",
    "        coluna_completa = bd_semNA.drop(colunas_parciais, axis = 1).reset_index().columns[0],\n",
    "        colunas_ignoradas = colunas_ignoradas\n",
    "    )\n",
    "    \n",
    "    if retorna_parciais == True:\n",
    "        return [bd_semNA, colunas_parciais, colunas_agrupadas, campos_com_erro]\n",
    "    else:\n",
    "        return [bd_semNA, colunas_agrupadas, campos_com_erro]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faz um gráfico de frequências com base na coluna escolhida (_plot_feature_freq_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_freq(bd, coluna, showing=30):\n",
    "    import matplotlib.ticker as mticker\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.subplot(121)\n",
    "\n",
    "    labels, counts = np.unique(bd[coluna].dropna(), return_counts=True)\n",
    "\n",
    "    # ordena pelas mais frequentes\n",
    "    p = np.argsort(counts)[::-1]\n",
    "    labels, counts = labels[p], counts[p]\n",
    "    \n",
    "    tabela_grafico = pd.DataFrame( data = [labels[:showing], counts[:showing]]).T\n",
    "    tabela_grafico.columns = [\"Labels\", \"Values\"]\n",
    "    # display(tabela_grafico)\n",
    "\n",
    "    g = sns.barplot(data = tabela_grafico, x = \"Labels\", y = \"Values\")\n",
    "    # g.set_xticks(tabela_grafico[\"Values\"])\n",
    "    g.xaxis.set_major_locator(mticker.FixedLocator(g.get_xticks()))\n",
    "    g.set_xticklabels(tabela_grafico[\"Labels\"], rotation=90)\n",
    "    \n",
    "    # return g\n",
    "\n",
    "# plot_feature_freq(bd_dados_completos, \"sector\", showing=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retorna caminho acima de alguma pasta, útil para quando o caminho contém planilhas para leitura, mas você quer exportar na pasta de cima (_retorna_caminho_pasta_acima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_caminho_pasta_acima(\n",
    "        caminho_pasta,\n",
    "        com_barra = False,\n",
    "        var_split = \"/\"\n",
    "        ):\n",
    "    caminho_acima = \"\"\n",
    "    for i in caminho_pasta.split(var_split):\n",
    "        if i == caminho_pasta.split(var_split)[-1]:\n",
    "            break\n",
    "        caminho_acima = caminho_acima + i + \"/\"\n",
    "\n",
    "    if com_barra == True:\n",
    "        return caminho_acima\n",
    "    else:\n",
    "        return caminho_acima[:-1]\n",
    "\n",
    "#### CHECKS\n",
    "display(os.getcwd())\n",
    "\n",
    "retorna_caminho_pasta_acima(\n",
    "        os.getcwd(),\n",
    "        com_barra = True,\n",
    "        var_split = \"\\\\\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar para o Excel, já com uma tabelinha pronta para fazer Pivot Tables (_exporta_excel_com_tabela_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exporta_excel_com_tabela(\n",
    "        bd,\n",
    "        caminho_nome_arquivo\n",
    "        ):\n",
    "    # https://stackoverflow.com/questions/58326392/how-to-create-excel-table-with-pandas-to-excel\n",
    "    # import pandas as pd\n",
    "\n",
    "    # Create a Pandas dataframe from some data.\n",
    "    # data = [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "    # df = pd.DataFrame({'Rank': data,\n",
    "                    # 'Country': data,\n",
    "                    # 'Population': data,\n",
    "                    # 'Data1': data,\n",
    "                    # 'Data2': data})\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(caminho_nome_arquivo, engine='xlsxwriter')\n",
    "\n",
    "    # Convert the dataframe to an XlsxWriter Excel object. Turn off the default\n",
    "    # header and index and skip one row to allow us to insert a user defined\n",
    "    # header.\n",
    "    bd.to_excel(writer, sheet_name='Sheet1', startrow=1, header=False, index=False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Get the dimensions of the dataframe.\n",
    "    (max_row, max_col) = bd.shape\n",
    "\n",
    "    # Create a list of column headers, to use in add_table().\n",
    "    column_settings = []\n",
    "    for header in bd.columns:\n",
    "        column_settings.append({'header': header})\n",
    "\n",
    "    # Add the table.\n",
    "    worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': column_settings})\n",
    "\n",
    "    # Make the columns wider for clarity.\n",
    "    worksheet.set_column(0, max_col - 1, 12)\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# exporta_excel_com_tabela(\n",
    "        # bd = dados_consolidado,\n",
    "        # caminho_nome_arquivo = caminho + \"Custo e Volume de Produção.xlsx\"\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizações rápidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograma padrão (_histograma_padrao_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograma_padrao(\n",
    "    dados_x,\n",
    "    titulo = np.nan,\n",
    "    dados_indice = np.nan,\n",
    "    tipo = \"hist\",\n",
    "    grid = False,\n",
    "    legend = True,\n",
    "    linha_media = np.nan,\n",
    "    qtd_bins = 10,\n",
    "    espessura = 0.8,\n",
    "    limite_x_min = np.nan, limite_x_max = np.nan,\n",
    "    limite_y_min = np.nan, limite_y_max = np.nan,\n",
    "    ):\n",
    "  \n",
    "    plt.figure(figsize = (20,8))\n",
    "\n",
    "    if not(pd.isnull(titulo)):\n",
    "      plt.title(titulo, fontsize = 16)\n",
    "\n",
    "    if tipo == \"hist\":\n",
    "        n, bins, edges = plt.hist(\n",
    "            dados_x,\n",
    "            bins = qtd_bins,\n",
    "            rwidth = espessura\n",
    "            )\n",
    "        plt.xticks(bins)\n",
    "        ax1 = plt.gca()\n",
    "        ax1.xaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:,.0f}\"))\n",
    "\n",
    "    elif tipo == \"bar\":\n",
    "        plt.bar(\n",
    "            dados_indice,\n",
    "            dados_x,\n",
    "            width = espessura\n",
    "            )\n",
    "\n",
    "  \n",
    "  \n",
    "    if not(math.isnan(linha_media)):\n",
    "        plt.axvline(x=linha_media, color = \"black\", label='Média')\n",
    "    \n",
    "    if legend == True:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.grid(grid)\n",
    "\n",
    "    if (not(math.isnan(limite_x_max))) & (not(math.isnan(limite_x_min))):\n",
    "      # print(limite_x_min)\n",
    "      # print(~math.isnan(limite_x_max))\n",
    "      plt.xlim(limite_x_min, limite_x_max)\n",
    "  \n",
    "    if (not(math.isnan(limite_y_max))) & (not(math.isnan(limite_y_min))):\n",
    "      plt.ylim(limite_y_min, limite_y_max)\n",
    "  \n",
    "    plt.show()\n",
    "\n",
    "    if tipo == \"hist\":\n",
    "        return n, bins, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico no Matplotlib com rótulos (_grafico_com_rotulos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafico_com_rotulos(\n",
    "    dados, coluna_x, coluna_y, \n",
    "    tipo_grafico = \"bar\", titulo_grafico = \"\", tamanho_grafico = (20, 8),\n",
    "    fig = None, ax = None, \n",
    "    tamanho_fonte = 12, grossura_barra = 1,\n",
    "    cor_x = \"b\", rotacao_x = 0, \n",
    "    formato_y = \":.2f\", rotacao_y = 0, limite_min_y = None, limite_max_y = None,\n",
    "    xytext_label_y = (0,10),\n",
    "  ):\n",
    "  \n",
    "  # dados_plot = dados_cliente[dados_cliente[\"% Faturamento Acumulado\"] < 0.40]\n",
    "\n",
    "  if fig is None:\n",
    "    fig = plt.figure(figsize = tamanho_grafico)\n",
    "  \n",
    "  plt.clf()\n",
    "  plt.rcParams.update({'font.size': tamanho_fonte})\n",
    "\n",
    "\n",
    "  ys = dados[coluna_y]\n",
    "  xs = np.arange(len(dados[coluna_x]))\n",
    "\n",
    "  if (tipo_grafico == \"bar\") | (tipo_grafico == \"line\"):\n",
    "    if (tipo_grafico == \"bar\"):\n",
    "      plt.bar(\n",
    "          x = xs,\n",
    "          height = ys,\n",
    "          width = grossura_barra,\n",
    "          label = coluna_y,\n",
    "      )\n",
    "    elif (tipo_grafico == \"line\"):\n",
    "      plt.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        cor_x + \"o-\",\n",
    "        label = coluna_y,\n",
    "      )\n",
    "\n",
    "    # display(cor_x + \"o-\")\n",
    "\n",
    "\n",
    "    ax1 = plt.gca()\n",
    "    if (limite_min_y is not None) & (limite_max_y is not None):\n",
    "      ax1.set_ylim([min(dados[coluna_y])*limite_min_y, max(dados[coluna_y])*limite_max_y])\n",
    "\n",
    "    ax1.set_ylabel(coluna_y, color = cor_x)\n",
    "    ax1.set_xlabel(coluna_x)\n",
    "\n",
    "    for x,y in zip(xs,ys):\n",
    "        formato_y_rotulo = \"{\" + formato_y + \"}\"\n",
    "        label = formato_y_rotulo.format(y)\n",
    "\n",
    "        plt.annotate(label, # this is the text\n",
    "                    (x,y), # these are the coordinates to position the label\n",
    "                    textcoords=\"offset points\", # how to position the text\n",
    "                    xytext = xytext_label_y, # distance from text to points (x,y)\n",
    "                    ha='center', # horizontal alignment can be left, right or center\n",
    "                    color = cor_x,\n",
    "                    rotation = rotacao_y)\n",
    "\n",
    "\n",
    "    ax1.xaxis.set_tick_params(rotation = rotacao_x)\n",
    "\n",
    "    # fig.legend()\n",
    "    plt.xticks(xs, dados[coluna_x])\n",
    "    formato_y_eixo = \"{x\" + formato_y + \"}\"\n",
    "    ax1.yaxis.set_major_formatter(ticker.StrMethodFormatter(formato_y_eixo))\n",
    "\n",
    "  elif tipo_grafico == \"barh\":\n",
    "    plt.barh(\n",
    "        y = xs,\n",
    "        width = ys,\n",
    "        height = grossura_barra,\n",
    "        # label = coluna_y,\n",
    "    )\n",
    "    ax1 = plt.gca()\n",
    "    ax1.set_yticks(xs, dados[coluna_x])\n",
    "\n",
    "    ax1.set_ylabel(coluna_x)\n",
    "    ax1.set_xlabel(coluna_y, color = cor_x)\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    if (limite_min_y is not None) & (limite_max_y is not None):\n",
    "      ax1.set_xlim([min(dados[coluna_y])*limite_min_y, max(dados[coluna_y])*limite_max_y])\n",
    "\n",
    "    for x,y in zip(xs,ys):\n",
    "      label = formato_y.format(y)\n",
    "\n",
    "      plt.annotate(label, # this is the text\n",
    "                (y, x), # these are the coordinates to position the label\n",
    "                textcoords=\"offset points\", # how to position the text\n",
    "                xytext = (xytext_label_y[1], xytext_label_y[0]), # distance from text to points (x,y)\n",
    "                ha='center', # horizontal alignment can be left, right or center\n",
    "                color = cor_x,\n",
    "                rotation = rotacao_y)\n",
    "\n",
    "  plt.title(titulo_grafico)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento de modelos (_treina_e_roda_modelo_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina_e_roda_modelo(\n",
    "    dados,\n",
    "    colunas_treino,\n",
    "    coluna_resultado,\n",
    "    \n",
    "    estimador = \"SVC\",\n",
    "    proporcao = 0.25,\n",
    "    SEED = 42,\n",
    "    print_tamanho = True,\n",
    "    print_score = True,\n",
    "    print_confusion_matrix = False,\n",
    "    \n",
    "    DummyClassifier__estrategia = None,\n",
    "    \n",
    "    DecisionTreeClassifier__retornar_visualizacao = False,\n",
    "    DecisionTreeClassifier__max_depth = None,\n",
    "    \n",
    "    RandomForestClassifier__n_estimators = 100,\n",
    "    \n",
    "    feature_selection = None,\n",
    "    scaler = None,\n",
    "):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    # from sklearn.svm import LinearSVC\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    \n",
    "    # from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import graphviz\n",
    "    import os\n",
    "    os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin' # https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "    # DIVIDE A BASE NAS PORÇÕES DE TESTE E TREINO, X E Y\n",
    "    [original_x_treino, original_x_teste, y_treino, y_teste] = train_test_split(\n",
    "        dados.loc[:, colunas_treino],\n",
    "        dados.loc[:, coluna_resultado],\n",
    "        test_size = proporcao,\n",
    "        stratify = dados.loc[:, coluna_resultado] # mesma proporção de y\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # SELECIONA AS FEATURES COM BASE NO ARGUMENTO E REESCALA SE NECESSÁRIO\n",
    "    if feature_selection is not None:\n",
    "        feature_selection.fit(original_x_treino, y_treino)\n",
    "        x_treino = feature_selection.transform(original_x_treino)\n",
    "        x_teste = feature_selection.transform(original_x_teste)\n",
    "        \n",
    "    if (scaler is None) | ((estimador is not None) * (estimador != \"DecisionTreeClassifier\")): # Não é necessário reescalar para árvore de decisão\n",
    "        if feature_selection is None:\n",
    "            x_treino = original_x_treino\n",
    "            x_teste = original_x_teste\n",
    "    else:\n",
    "        if feature_selection is None:\n",
    "            scaler.fit(original_x_treino)\n",
    "            x_treino = scaler.transform(original_x_treino)\n",
    "            x_teste = scaler.transform(original_x_teste)\n",
    "        else:\n",
    "            scaler.fit(x_treino)\n",
    "            x_treino = scaler.transform(x_treino)\n",
    "            x_teste = scaler.transform(x_teste)\n",
    "\n",
    "\n",
    "    # IMPRIME O TAMANHO DE CADA PORÇÃO\n",
    "    if print_tamanho == True:\n",
    "        print(\"Tamanho treino: \" + str(len(x_treino)))\n",
    "        print(\"Tamanho teste: \" + str(len(x_teste)))\n",
    "\n",
    "\n",
    "    # INSTANCIA O ESTIMADOR ESCOLHIDO. SE NÃO HOUVER ESTIMADOR, RETORNA APENAS AS PORÇÕES SELECIONADAS E REESCALADAS\n",
    "    if estimador is None:\n",
    "        return [\n",
    "            [original_x_treino, original_x_teste, y_treino, y_teste], \n",
    "            [x_treino, x_teste],\n",
    "            feature_selection\n",
    "        ]  \n",
    "    elif estimador == \"SVC\":\n",
    "        modelo = SVC(gamma = \"auto\")\n",
    "    \n",
    "    elif estimador == \"RandomForestClassifier\":\n",
    "        modelo = RandomForestClassifier(n_estimators = RandomForestClassifier__n_estimators)\n",
    "\n",
    "    elif estimador == \"DecisionTreeClassifier\":\n",
    "        if DecisionTreeClassifier__max_depth is None:\n",
    "            modelo = DecisionTreeClassifier()\n",
    "        else:\n",
    "            modelo = DecisionTreeClassifier(max_depth = DecisionTreeClassifier__max_depth)\n",
    "\n",
    "    elif estimador == \"MultinomialNB\":\n",
    "        modelo = MultinomialNB()\n",
    "        \n",
    "    elif estimador == \"Dummy\":\n",
    "        if DummyClassifier__estrategia is None:  \n",
    "            modelo = DummyClassifier()\n",
    "        else:\n",
    "            modelo = DummyClassifier(strategy = DummyClassifier__estrategia)\n",
    "        \n",
    "    else:\n",
    "        return print(\"Estimador \" + estimador + \" não encontrado\")\n",
    "    \n",
    "    \n",
    "    # TREINA O ESTIMADOR ESCOLHIDO\n",
    "    try:\n",
    "        modelo.fit(x_treino, y_treino)\n",
    "    except ValueError:\n",
    "        return print(\"ValueError\")\n",
    "\n",
    "    # AVALIA O MODELO\n",
    "    \n",
    "    # previsoes = modelo.predict(x_teste)\n",
    "    # taxa_de_acerto = accuracy_score(y_teste, previsoes)\n",
    "\n",
    "    taxa_de_acerto = modelo.score(x_teste, y_teste)\n",
    "    \n",
    "    if print_score == True:\n",
    "        print(\"Taxa de acerto do modelo \" + estimador + \": {:.2%}\".format(taxa_de_acerto))\n",
    "        \n",
    "    if print_confusion_matrix == True:\n",
    "        matriz_confusao = confusion_matrix(y_teste, modelo.predict(x_teste))\n",
    "        # print(matriz_confusao)\n",
    "        sns.set(font_scale = 2)\n",
    "        sns.heatmap(matriz_confusao, annot = True, fmt = \"d\").set(xlabel = \"Predição\", ylabel = \"Real\")\n",
    "        plt.show()\n",
    "    \n",
    "    # RETORNA VISÃO GRÁFICA PARA O DECISION TREE CLASSIFIER\n",
    "    if (DecisionTreeClassifier__retornar_visualizacao == True) * (estimador == \"DecisionTreeClassifier\"):\n",
    "        # return export_graphviz(modelo, out_file = None)\n",
    "        dot_data = export_graphviz(\n",
    "            modelo, \n",
    "            feature_names = colunas_treino,\n",
    "            filled = True,\n",
    "            rounded = True,\n",
    "            class_names = [\"Não\", \"Sim\"]\n",
    "        )\n",
    "        grafico = graphviz.Source(dot_data)\n",
    "        return grafico\n",
    "    \n",
    "    # RETORNA BASES E MODELO PARA OUTROS ESTIMADORES\n",
    "    return [\n",
    "        modelo, \n",
    "        [original_x_treino, original_x_teste, y_treino, y_teste], \n",
    "        [x_treino, x_teste], \n",
    "        feature_selection,\n",
    "        # previsoes, \n",
    "        taxa_de_acerto\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch padronizado (_treina_modelo_grid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina_modelo_grid(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  modelo,\n",
    "  param_grid,\n",
    "  cv = 10,\n",
    "  scoring = \"neg_mean_absolute_error\",\n",
    "  tipo = \"grid\",\n",
    "  random_state_seed = 1082141,\n",
    "  n_iter = 50\n",
    "):\n",
    "  from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "  \n",
    "  if tipo == \"grid\":\n",
    "      grid = GridSearchCV(\n",
    "          modelo, \n",
    "          param_grid, \n",
    "          cv = cv, \n",
    "          scoring = scoring, \n",
    "          return_train_score = True,\n",
    "      )\n",
    "\n",
    "  elif tipo == \"randomized\":\n",
    "      # print(\"Busca randomizada em \" + str(n_iter) + \" combinações de parâmetros.\")\n",
    "      grid = RandomizedSearchCV(\n",
    "          modelo, \n",
    "          param_grid, \n",
    "          cv = cv, \n",
    "          scoring = scoring, \n",
    "          return_train_score = True,\n",
    "          random_state = random_state_seed,\n",
    "          n_iter = n_iter,\n",
    "      )\n",
    "\n",
    "  grid.fit(x_train, y_train)\n",
    "\n",
    "  return grid\n",
    "\n",
    "### Definimos range de hiperparâmetros\n",
    "# n_neighbors_range = range(10, 20)\n",
    "# metric_range = [\"cityblock\", \"cosine\", \"euclidean\", \"haversine\", \"l1\", \"l2\", \"manhattan\", \"nan_euclidean\", \"minkowski\"]\n",
    "# algorithm_range = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "\n",
    "# param_grid = dict(algorithm = algorithm_range)\n",
    "# # display(param_grid) \n",
    "\n",
    "# grid_melhor_algorithm = treina_modelo_grid(\n",
    "#     dados_treino_X,\n",
    "#     dados_treino_y,\n",
    "#     KNeighborsClassifier(n_neighbors = 12, metric = \"euclidean\"),\n",
    "#     param_grid,\n",
    "#     cv = 5,\n",
    "#     scoring = \"accuracy\",\n",
    "#     tipo = \"grid\",\n",
    "#     random_state_seed = numero_aleatorio,\n",
    "#     # n_iter = 50\n",
    "# )\n",
    "\n",
    "# # print(grid.best_score_)\n",
    "# # grid.best_estimator_\n",
    "# tabela_resultados = pd.concat([\n",
    "#   pd.json_normalize(pd.DataFrame(grid_melhor_algorithm.cv_results_)[\"params\"]),\n",
    "#   pd.DataFrame(grid_melhor_algorithm.cv_results_)[[\"rank_test_score\", \"mean_test_score\"]]\n",
    "# ], axis = 1).set_index(\"rank_test_score\").sort_index()\n",
    "\n",
    "\n",
    "# plt.figure(figsize = (15,8))\n",
    "# display(tabela_resultados)\n",
    "# plt.bar(x = tabela_resultados[\"algorithm\"], height = tabela_resultados[\"mean_test_score\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finanças, Day Trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão linear com pontos de uma ação (_criar_regressao_bd_acao_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_regressao_bd_acao(\n",
    "  bd_acao,\n",
    "  coluna = \"Close\",\n",
    "  print_variaveis = False,\n",
    "  plot_grafico = False,\n",
    "  tamanho_figsize = (10,5),\n",
    "  rotacao = 45,\n",
    "  titulo = \"Pontos no fechamento da ação\",\n",
    "  var_pular_final_de_semana_feriados = False,\n",
    "):\n",
    "  # bd_acao\n",
    "\n",
    "  bd_acao_coluna = bd_acao.reset_index()[[bd_acao.index.name, coluna]]\n",
    "  # bd_acao_coluna\n",
    "\n",
    "  # from sklearn.linear_model import LinearRegression\n",
    "\n",
    "  X = bd_acao_coluna.reset_index()[\"index\"].array.reshape(-1, 1)\n",
    "  y = bd_acao_coluna.loc[:, coluna].array.reshape(-1, 1)\n",
    "\n",
    "  modelo_linear = LinearRegression()\n",
    "  modelo_linear.fit(X, y)\n",
    "\n",
    "  bd_acao_coluna.loc[:, \"Regressão\"] = modelo_linear.predict(X)\n",
    "  # bd_acao_fim\n",
    "\n",
    "  desvio_padrao = bd_acao_coluna[coluna].std()\n",
    "  media = bd_acao_coluna[coluna].mean()\n",
    "  alfa = modelo_linear.coef_[0][0]\n",
    "  valor_fechamento = bd_acao_coluna.sort_index(ascending = False).iloc[0][coluna]\n",
    "\n",
    "  # margem = 0.005\n",
    "  margem = desvio_padrao/media\n",
    "\n",
    "  if print_variaveis == True:\n",
    "    display(\"Média: \" + \"{:.4f}\".format(media))\n",
    "    display(\"Desvio padrão: \" + \"{:.4f}\".format(desvio_padrao))\n",
    "    display(\"Desvio padrão (%): \" + \"{:.4f}\".format(margem))\n",
    "    display(\"Inclinação da reta (alfa, coeficiente angular): \" + \"{:.4f}\".format(alfa))\n",
    "    display(\"Valor de fechamento (R$): \" + \"{:.2f}\".format(valor_fechamento))\n",
    "\n",
    "\n",
    "  if plot_grafico == True:\n",
    "    plt.figure(figsize = tamanho_figsize)\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    if var_pular_final_de_semana_feriados == True:\n",
    "      ax.xaxis.set_major_locator(ticker.LinearLocator(len(bd_acao_coluna)))\n",
    "      # ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "      # ax.xaxis.set_major_formatter(mdates.DateFormatter(fmt = \"%d/%m/%y\"))\n",
    "\n",
    "      plt.xticks(rotation = rotacao)\n",
    "\n",
    "      plt.scatter(x = bd_acao_coluna[bd_acao.index.name].dt.strftime(\"%d/%m/%y\").astype(str), y = coluna, data = bd_acao_coluna, edgecolors='black', facecolors='none')\n",
    "\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name].dt.strftime(\"%d/%m/%y\").astype(str), bd_acao_coluna[\"Regressão\"]-desvio_padrao, color = \"blue\", linestyle='dashed')\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name].dt.strftime(\"%d/%m/%y\").astype(str), bd_acao_coluna[\"Regressão\"], color='green')\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name].dt.strftime(\"%d/%m/%y\").astype(str), bd_acao_coluna[\"Regressão\"]+desvio_padrao, color = \"blue\", linestyle='dashed')\n",
    "\n",
    "      # ax.xaxis.set_major_formatter(mdates.DateFormatter(fmt = \"%d/%m/%y\"))\n",
    "      # ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "\n",
    "    else:\n",
    "      ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "\n",
    "      plt.xticks(bd_acao_coluna[bd_acao.index.name], rotation = rotacao)\n",
    "\n",
    "      plt.scatter(x = bd_acao_coluna[bd_acao.index.name], y = coluna, data = bd_acao_coluna, edgecolors='black', facecolors='none')\n",
    "\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name], bd_acao_coluna[\"Regressão\"]-desvio_padrao, color = \"blue\", linestyle='dashed')\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name], bd_acao_coluna[\"Regressão\"], color='green')\n",
    "      plt.plot(bd_acao_coluna[bd_acao.index.name], bd_acao_coluna[\"Regressão\"]+desvio_padrao, color = \"blue\", linestyle='dashed')\n",
    "\n",
    "\n",
    "    plt.title(titulo)\n",
    "    plt.show()\n",
    "\n",
    "  return [bd_acao_coluna, media, desvio_padrao, modelo_linear, valor_fechamento]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candle Plot (_candle_plot_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candle_plot(\n",
    "    dados,\n",
    "    volume = True,\n",
    "    mav = np.nan,\n",
    "    colors = [\"orange\", \"yellow\", \"blue\"],\n",
    "    titulo = \"\",\n",
    "    ):\n",
    "\n",
    "  # !python -m pip install plotly\n",
    "  from plotly.subplots import make_subplots\n",
    "  import plotly.graph_objects as go\n",
    "  \n",
    "  if volume == True:\n",
    "    fig = make_subplots(\n",
    "        rows = 2,\n",
    "        cols = 1,\n",
    "        shared_xaxes = True,\n",
    "        vertical_spacing = 0.1,\n",
    "        subplot_titles = (\"Candlesticks\", \"Volume transacionado\"),\n",
    "        row_width = [0.2, 0.7]\n",
    "    )\n",
    "  else:\n",
    "    fig = make_subplots(\n",
    "        rows = 1,\n",
    "        cols = 1,\n",
    "        shared_xaxes = True,\n",
    "        vertical_spacing = 0.1,\n",
    "        subplot_titles = (\"Candlesticks\"),\n",
    "        row_width = [0.2, 0.7]\n",
    "    )\n",
    "\n",
    "  fig.add_trace(go.Candlestick(x=dados.index,\n",
    "                      open = dados['Open'],\n",
    "                      high = dados['High'],\n",
    "                      low = dados['Low'],\n",
    "                      close = dados['Close']),\n",
    "                row = 1, col = 1)\n",
    "\n",
    "  if mav is not np.nan:\n",
    "    for i in range(len(mav)):\n",
    "      # print(i)\n",
    "      dados[\"Close \"+ str(mav[i]) +\" dias\"] = dados[\"Close\"].rolling(window=mav[i]).mean()\n",
    "      fig.add_trace(go.Scatter(x=dados.index,\n",
    "                          y = dados[\"Close \"+ str(mav[i]) +\" dias\"],\n",
    "                          mode = \"lines\",\n",
    "                          name = \"Média móvel fechamento \" + str(mav[i]) + \" dias\",\n",
    "                          marker=dict(color=colors[i])),\n",
    "                    row = 1, col = 1)\n",
    "\n",
    "  if volume == True:\n",
    "    fig.add_trace(go.Bar(x=dados.head(60).index,\n",
    "                        y = dados['Volume'],\n",
    "                        name = \"Volume\"),\n",
    "                  row = 2, col = 1)\n",
    "\n",
    "\n",
    "  fig.update_layout(\n",
    "      yaxis_title = \"Preço\",\n",
    "      xaxis_rangeslider_visible=False,\n",
    "      title=titulo,\n",
    "      )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "# candle_plot(\n",
    "#     dados = dados_candle_matp_media_movel.head(60),\n",
    "#     volume = True,\n",
    "#     mav = [7,14],\n",
    "#     # colors = [\"orange\", \"yellow\", \"blue\"],\n",
    "#     titulo = \"PETR4.SA\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plota candlesticks e acha martelos (_plota_candlestick_acha_martelos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plota_candlestick_acha_martelos(\n",
    "    acao,\n",
    "    periodo = \"21d\",\n",
    "    intervalo = \"1d\",\n",
    "    taxa_máxima_para_ser_martelo = 0.2,\n",
    "    display_tabela_martelo = True,\n",
    "    display_candlestick = True,\n",
    "\n",
    "  ):\n",
    "\n",
    "  bd_acao = yf.Ticker(acao).history(\n",
    "      period = periodo,\n",
    "      interval = intervalo\n",
    "  )\n",
    "  bd_acao[\"Amplitude Open-Close\"] = abs(bd_acao[\"Open\"] - bd_acao[\"Close\"])\n",
    "  bd_acao[\"Amplitude High-Low\"] = abs(bd_acao[\"High\"] - bd_acao[\"Low\"])\n",
    "\n",
    "  # taxa_máxima_para_ser_martelo = 0.2\n",
    "  bd_acao[\"Martelo?\"] = (bd_acao[\"Amplitude Open-Close\"] < taxa_máxima_para_ser_martelo * bd_acao[\"Amplitude High-Low\"])\n",
    "\n",
    "  lista_datas_martelo = bd_acao[bd_acao[\"Martelo?\"] == True].sort_index(ascending = False).index.to_pydatetime()\n",
    "  string_datas_martelo = \"\"\n",
    "  for data in lista_datas_martelo:\n",
    "    string_datas_martelo = data.strftime(\"%d/%m/%y\") + \", \" + string_datas_martelo\n",
    "\n",
    "  # string_datas_martelo[:-2]\n",
    "\n",
    "\n",
    "  if display_tabela_martelo == True:\n",
    "    display(bd_acao[bd_acao[\"Martelo?\"] == True])\n",
    "\n",
    "  if display_candlestick == True:\n",
    "    candle_plot(\n",
    "    bd_acao,\n",
    "    volume = True,\n",
    "    # mav = np.nan,\n",
    "    # colors = [\"orange\", \"yellow\", \"blue\"],\n",
    "    titulo = acao,\n",
    "    )\n",
    "\n",
    "  return [bd_acao, string_datas_martelo]\n",
    "\n",
    "[_, string_datas_martelo] = plota_candlestick_acha_martelos(\n",
    "    acao =  \"PETR3\" + \".SA\",\n",
    "    periodo = \"60d\",\n",
    "    intervalo = \"1d\",\n",
    "    taxa_máxima_para_ser_martelo = 0.2,\n",
    "    display_tabela_martelo = True,\n",
    "    display_candlestick = True,\n",
    "  );\n",
    "\n",
    "display(string_datas_martelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpa memória - NÃO FUNCIONA (_retrieve_name_, _limpa_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_name(var):\n",
    "    import inspect\n",
    "\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    \n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var][0]\n",
    "\n",
    "def limpa(ponteiro):\n",
    "    try:\n",
    "        del(ponteiro)\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    except:\n",
    "        print(\"Não foi possível limpar \"+ retrieve_name(ponteiro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho_pasta = r\"C:/Users/ricardopeloi/...\"\n",
    "# arquivo = \"abc.xlsx\"\n",
    "\n",
    "# bd_arquivo = pd.ExcelFile(caminho_pasta + \"\\\\\" + arquivo)\n",
    "# bd_arquivo.sheet_names\n",
    "\n",
    "# bd = pd.read_excel(caminho_pasta + \"\\\\\" + arquivo, sheet_name = 'aba')\n",
    "# bd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise exploratória"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação treino e teste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação/Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
